111/125
the magnitudes of the parameters can increase or decrease exponentially in forward and backward(magnitude of the gradients) passes if not initialized correctly
vanishing gradient: weights are too small so gradient becomes smaller until we can't get enough precision.
exploding gradient: weights are too large and get larger so gradient becomes unstable and hard to train

just read the book again I cannot summarize